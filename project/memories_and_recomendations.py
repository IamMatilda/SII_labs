from collections import Counter
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from db_operations import fetch_messages  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
import os

os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('russian'))

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
additional_stopwords = {
    "–∏", "–≤", "–≤–æ", "–Ω–µ", "—á—Ç–æ", "–æ–Ω", "–Ω–∞", "—è", "—Å", "—Å–æ", "–∫–∞–∫", "–¥–∞", "—Ç–∞–∫", "–≤–æ—Ç", "–Ω–æ", "–¥–ª—è", "–∂–µ", "—Ç—ã", "–±—ã", "–ø–æ", "—Ç–æ–ª—å–∫–æ", "–µ–µ", "–º–Ω–µ", "—É", "–æ—Ç", "–∏–∑", "–º—ã", "–≤—ã", "–∏—Ö", "—ç—Ç–æ", "–∫", "–≤—Å–µ—Ö", "–≤–µ—Å—å", "–µ—â–µ", "–¥–∞–∂–µ", "–µ—Å–ª–∏", "–ø—Ä–∏", "–º–µ–Ω—è", "–æ", "—Ç–∞–∫–æ–π", "—Å–∞–º", "–±—É–¥–µ—Ç", "–±–µ–∑", "–∏–ª–∏", "–≤—Å—é", "–æ–Ω–∞", "–∫—Ç–æ", "—Ç–æ—Ç", "—ç—Ç–æ—Ç", "–º–æ—á—å", "–Ω–∏—á–µ–≥–æ", "—ç—Ç–∏", "—ç—Ç–æ–≥–æ", "–º–æ–∏", "—Ç–µ–ø–µ—Ä—å", "–∫–æ–≥–¥–∞", "–∫—É–¥–∞", "–º–µ–∂–¥—É", "–¥—Ä—É–≥–∏–µ", "–≤–∞—Å", "—Ç–æ–∂–µ", "—Ç–∞–∫–∂–µ"
}
stop_words.update(additional_stopwords)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def clean_message(message):
    message = re.sub(r'[^\w\s]', '', message)
    message = message.lower()
    words = word_tokenize(message)
    filtered_words = [word for word in words if word not in stop_words]
    return " ".join(filtered_words)

# –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
messages_data = fetch_messages(limit=15)
if not messages_data:
    print("–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    exit()

# –û—á–∏—Å—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
cleaned_messages = [clean_message(item["text"]) for item in messages_data]

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ GPT-2
model_name = "sberbank-ai/rugpt3small_based_on_gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
sentiment_analyzer = pipeline("sentiment-analysis", model="blanchefort/rubert-base-cased-sentiment")
sentiments = [sentiment_analyzer(msg)[0] for msg in cleaned_messages]

# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
context = "–í–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö:\n"
for i, (msg, sentiment) in enumerate(zip(cleaned_messages, sentiments)):
    context += f"{i + 1}. {msg} (–°–æ—Å—Ç–æ—è–Ω–∏–µ: {sentiment['label']})\n"

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
input_ids = tokenizer.encode(context, return_tensors="pt")
output = model.generate(
    input_ids,
    max_length=800,
    temperature=0.8,
    top_p=0.85,
    repetition_penalty=1.2
)

generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π:\n", generated_text)

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
vectorizer = TfidfVectorizer(stop_words=None)
tfidf_matrix = vectorizer.fit_transform(cleaned_messages)
num_clusters = 3
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
labels = kmeans.fit_predict(tfidf_matrix)

# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
clustered_messages = {i: [] for i in range(num_clusters)}
for idx, label in enumerate(labels):
    clustered_messages[label].append(messages_data[idx]["text"])

print("\n–ö–ª–∞—Å—Ç–µ—Ä—ã —Å–æ–æ–±—â–µ–Ω–∏–π:")
for cluster, msgs in clustered_messages.items():
    print(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster}: {msgs}")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
topic_mapping = {
    "—Å–ø–æ—Ä—Ç": ["—Å–ø–æ—Ä—Ç", "–∏–≥—Ä–∞", "–∫–æ–º–∞–Ω–¥–∞", "–ø–æ–±–µ–¥–∞"],
    "–ø–æ–ª–∏—Ç–∏–∫–∞": ["–ø–æ–ª–∏—Ç–∏–∫–∞", "–≤–ª–∞—Å—Ç—å", "–Ω–æ–≤–æ—Å—Ç–∏", "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ"],
    "—Ñ–∏–Ω–∞–Ω—Å—ã": ["–¥–µ–Ω—å–≥–∏", "–∞–∫—Ü–∏–∏", "–¥–æ–ª–ª–∞—Ä", "—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
    "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è": ["–æ—Ç–¥—ã—Ö", "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è", "–∞–∫–≤–∞–ø–∞—Ä–∫", "—Ç—É—Ä—Ü–∏—è"],
    "–∑–¥–æ—Ä–æ–≤—å–µ": ["–∑–¥–æ—Ä–æ–≤—å–µ", "–≥–æ–ª–æ–≤–Ω–∞—è", "—Ç–∞–±–ª–µ—Ç–∫–∏"]
}

recommendations = set()
for cluster, msgs in clustered_messages.items():
    for msg in msgs:
        for topic, words in topic_mapping.items():
            if any(word in msg.lower() for word in words):
                recommendations.add(topic)

print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
print(list(recommendations))

# –°–±–æ—Ä –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
feedback = {"positive": [], "negative": []}
print("\n–û—Ü–µ–Ω–∏—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
for rec in recommendations:
    print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {rec}")
    user_input = input("–û—Ü–µ–Ω–∏—Ç–µ (üëç –∏–ª–∏ üëé): ")
    if user_input == "üëç":
        feedback["positive"].append(rec)
    elif user_input == "üëé":
        feedback["negative"].append(rec)

print("\n–°–æ–±—Ä–∞–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å:")
print("üëç –ü–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å:", feedback["positive"])
print("üëé –ù–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å:", feedback["negative"])

# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
preferences = {topic: 1 for topic in topic_mapping.keys()}
for rec in feedback["positive"]:
    preferences[rec] += 1
for rec in feedback["negative"]:
    preferences[rec] -= 1

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å —É—á–µ—Ç–æ–º –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
new_recommendations = [
    (f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –≤–∞–º –±–æ–ª—å—à–µ —É–∑–Ω–∞—Ç—å –ø—Ä–æ {topic}.", preferences[topic])
    for topic in preferences
]
sorted_recommendations = sorted(new_recommendations, key=lambda x: x[1], reverse=True)

print("\n–ù–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
for rec, weight in sorted_recommendations:
    print(f"{rec} (–≤–µ—Å: {weight})")